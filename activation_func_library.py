import numpy as np
import matplotlib.pylab as plt
# 活性化関数ライブラリ


# ステップ関数
# パーセプトロンで使用
# 信号が急に変更する活性化関数
def step_function(x):
    return np.array(x > 0, dtype=np.int)

# シグモイド関数
# 入力信号を曲線にして出力する活性化関数(信号がなめらかに変更する関数)
# ニューラルネットワークで使用
# 2クラス分類問題で使用するのが一般的
def sigmoid(x):
    return 1 / (1+np.exp(-x))

# 恒等関数
# 入力信号をそのまま出力する活性化関数
# 回帰問題の出力層の活性化関数として使用するのが一般的
def identity_function(x):
    return x

# ソフトマックス関数
# y = exp(x)/sum(exp(x))
# 補足：実際には、指数計算(exp)を行うと、すぐに表現できない値まで大きくなってしまう(オーバーフロー)
#      そのため、入力信号の各ニューロンに何らかの定数を引き算(もしくは足し算)して、値を小さくする。
#      exp(x+定数C)/sum(exp(x+定数C))
#      定数は入力信号の中で一番大きい値にするのが一般的
# 出力信号の各ニューロンに対して、全ての入力信号が影響を与える
# 出力値は0〜1.0の間の実数になり、出力の総和は1になる。
# ↑の性質のおかげで、ソフトマックスの出力を確率として解釈することができる。
# 注意点：ソフトマックスを適用しても、各要素の大小関係に影響はない。指数関数が単調増加する関数であるため。
# 分類問題の出力層の活性化関数として使用するのが一般的
# ★要素の大小関係が変わらないなら、出力層にソフトマックス適用しようがしまいが、判定結果に変わりはない。
#  出力層にソフトマックスが必要な理由はニューラルネットワークの学習に関係がある(TODO)
def softmax(x):
    c = np.max(x)
    exp_x = np.exp(x - c)
    sum_exp_x = np.sum(exp_x)
    y = exp_x / sum_exp_x
    return y

# ReLU関数
def ReLU(x):
    return np.maximum(0,x)